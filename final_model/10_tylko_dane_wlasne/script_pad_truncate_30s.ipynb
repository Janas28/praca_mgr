{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, MaxPooling1D, Dropout, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definicja modelu\n",
    "model = Sequential()\n",
    "model.add(Conv1D(2048, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(52, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Funkcje pomocnicze\n",
    "def add_noise(data, x):\n",
    "    noise = np.random.randn(len(data))\n",
    "    data_noise = data + x * noise\n",
    "    return data_noise\n",
    "\n",
    "def shift(data, x):\n",
    "    return np.roll(data, x)\n",
    "\n",
    "def stretch(data, rate):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "\n",
    "def pitch_shift(data, rate):\n",
    "    return librosa.effects.pitch_shift(data, sr=22050, n_steps=rate)\n",
    "\n",
    "def pad_or_truncate(data, target_length):\n",
    "    \"\"\"Funkcja do paddingu lub przycięcia sygnału audio do określonej długości.\"\"\"\n",
    "    current_length = len(data)\n",
    "    if current_length < target_length:\n",
    "        # Padding do określonej długości\n",
    "        padded_data = np.pad(data, (0, target_length - current_length), 'constant')\n",
    "    else:\n",
    "        # Przycięcie do określonej długości\n",
    "        padded_data = data[:target_length]\n",
    "    return padded_data\n",
    "\n",
    "def mfcc_feature_extraction_rr(dir_, target_length=30):\n",
    "    \"\"\"Funkcja do ekstrakcji cech z sygnałów audio z dodaniem paddingu/przycinania do target_length sekund.\"\"\"\n",
    "    X_ = []\n",
    "    y_ = []\n",
    "    data = df\n",
    "    features = 52  # Liczba cech MFCC\n",
    "\n",
    "    # Przetwarzanie wszystkich plików .wav\n",
    "    for soundDir in os.listdir(dir_):\n",
    "        if soundDir.endswith('.wav'):\n",
    "            label = list(data[data['filename'] == (soundDir[:-4])]['cycles'])[0]\n",
    "            data_x, sampling_rate = librosa.load(os.path.join(dir_, soundDir), sr=22050)\n",
    "\n",
    "            # Przeliczamy target_length na próbki\n",
    "            target_samples = int(target_length * sampling_rate)\n",
    "            \n",
    "            # Sprawdź długość dźwięku i pominij pliki dłuższe niż 30 sekund\n",
    "            if len(data_x) > target_samples:\n",
    "                print(f\"Pomijanie pliku {soundDir} (dłuższy niż {target_length} sekund)\")\n",
    "                continue\n",
    "\n",
    "            # Pad lub przycięcie sygnału do target_samples\n",
    "            data_x = pad_or_truncate(data_x, target_samples)\n",
    "\n",
    "            # Ekstrakcja MFCC\n",
    "            mfccs = librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=features)\n",
    "            mfccs_mean = np.mean(mfccs.T, axis=0)  # Średnia MFCC dla każdego wymiaru\n",
    "\n",
    "            # Dodanie cech i etykiet do list\n",
    "            X_.append(mfccs_mean)\n",
    "            y_.append(label)\n",
    "            \n",
    "            # Generowanie danych augmentowanych\n",
    "            data_shift = shift(data_x, 1600)\n",
    "            mfccs_shift = np.mean(librosa.feature.mfcc(y=data_shift, sr=sampling_rate, n_mfcc=features).T, axis=0)\n",
    "            X_.append(mfccs_shift)\n",
    "            y_.append(label)\n",
    "\n",
    "            data_noise = add_noise(data_x, 0.005)\n",
    "            mfccs_noise = np.mean(librosa.feature.mfcc(y=data_noise, sr=sampling_rate, n_mfcc=features).T, axis=0)\n",
    "            X_.append(mfccs_noise)\n",
    "            y_.append(label)\n",
    "\n",
    "    # Zduplikowanie danych\n",
    "    X_data = np.tile(np.array(X_), (duplication_factor, 1))\n",
    "    y_data = np.tile(np.array(y_), duplication_factor)\n",
    "    \n",
    "    return X_data, y_data\n",
    "\n",
    "# Ładowanie danych\n",
    "root = '../own_data_train'\n",
    "file_list = os.listdir(root)\n",
    "data = []\n",
    "\n",
    "for filename in file_list:\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    if match:\n",
    "        integer = int(match.group())\n",
    "        data.append({'cycles': integer, 'filename': filename[:-4]})\n",
    "\n",
    "df = pd.DataFrame(data, columns=['cycles', 'filename'])\n",
    "\n",
    "# Wywołanie funkcji ekstrakcji cech\n",
    "audio_data = root + \"/\"\n",
    "res_data, res_y = mfcc_feature_extraction_rr(audio_data)\n",
    "\n",
    "# Podział danych na zbiory treningowe i walidacyjne\n",
    "x_train, x_val, y_train, y_val = train_test_split(res_data, res_y, test_size=0.2, random_state=10)\n",
    "\n",
    "# Reshape danych wejściowych do kształtu oczekiwanego przez model (batch_size, timesteps, input_dim)\n",
    "x_train_lstm = np.expand_dims(x_train, axis=2)\n",
    "x_val_lstm = np.expand_dims(x_val, axis=2)\n",
    "\n",
    "# Kompilacja modelu\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "model.compile(optimizer=optimiser, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Trening modelu\n",
    "history = model.fit(x_train_lstm, y_train, batch_size=8, epochs=2500, validation_data=(x_val_lstm, y_val))\n",
    "\n",
    "# Zapisz model\n",
    "model.save(\"model_different_length.h5\")\n",
    "model.save_weights('model_different_length.weights.h5')\n",
    "\n",
    "# Zapisz historię treningu\n",
    "import pickle\n",
    "with open('model_different_length', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
